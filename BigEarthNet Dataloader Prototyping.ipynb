{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"../SmallEarthNet/\"\n",
    "cloud_shadow_file = \"../patches_with_cloud_and_shadow.csv\"\n",
    "snow_file = \"../patches_with_seasonal_snow.csv\"\n",
    "patches = os.listdir(data_dir)\n",
    "patches.sort()\n",
    "print(len(patches))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import load_data_tf as load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data.BigEarthNetDataset(split_file=\"splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 20,\n",
       " 22,\n",
       " 24,\n",
       " 25,\n",
       " 28,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dataset.key_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1839/9607 [00:00<00:00, 18385.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading train-val-test split cache from smallearthnet_splits.pkl\n",
      "File smallearthnet_splits.pkl not found. Creating new split instead with file name splits.pkl\n",
      "Building new train-val-test split and saving to splits.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9607/9607 [00:00<00:00, 17132.99it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_dataset = load_data.MetaBigEarthNetTaskDataset(data_dir=\"../SmallEarthNet\", split_file=\"smallearthnet_splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 19, 20, 22, 24, 25, 28, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42] 2965\n",
      "[[[1. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [1. 0. 1.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 1.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [0. 0. 1.]]] [[[ 4.  7. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]]\n",
      "\n",
      " [[ 4. -1. -1.]\n",
      "  [ 4.  6. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 7. -1. -1.]]\n",
      "\n",
      " [[ 4. -1. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [14. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [14. -1. -1.]\n",
      "  [14. -1. -1.]\n",
      "  [ 4. -1. -1.]]\n",
      "\n",
      " [[ 5.  6. 13.]\n",
      "  [ 5. -1. -1.]\n",
      "  [ 5. -1. -1.]\n",
      "  [ 5.  6. -1.]\n",
      "  [ 5. -1. -1.]\n",
      "  [ 5. -1. -1.]\n",
      "  [ 5. -1. -1.]\n",
      "  [ 5. -1. -1.]]\n",
      "\n",
      " [[ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]]\n",
      "\n",
      " [[ 5.  7. 13.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]]\n",
      "\n",
      " [[ 7. -1. -1.]\n",
      "  [ 4. 15. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]]\n",
      "\n",
      " [[ 4.  7. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [ 7. -1. -1.]]]\n"
     ]
    }
   ],
   "source": [
    "X, y, yt = meta_dataset.sample_batch(batch_size=16, split='train')\n",
    "print(y, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 120, 120]), torch.Size([16, 43]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = DataLoader(data, batch_size=16)\n",
    "img_batch, label_batch = next(iter(train))\n",
    "img_batch.size(), label_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading train-val-test split cache from splits.pkl\n",
      "Reloading train-val-test split cache from splits.pkl\n",
      "Reloading train-val-test split cache from splits.pkl\n"
     ]
    }
   ],
   "source": [
    "train, _, _ = load_data.get_dataloaders(split_file=\"splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8, 3, 120, 120]), torch.Size([8, 8, 3]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch, label_batch, raw_labels = next(iter(train))\n",
    "img_batch.size(), label_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  19,\n",
       "  20,\n",
       "  22,\n",
       "  24,\n",
       "  25,\n",
       "  28,\n",
       "  30,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42],\n",
       " tensor([[[22, -1, -1],\n",
       "          [ 4, 17, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [17, 22, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, 22, -1]],\n",
       " \n",
       "         [[ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, 37, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [22, -1, -1],\n",
       "          [ 4, 22, -1]],\n",
       " \n",
       "         [[ 4,  5, 22],\n",
       "          [ 4,  5, -1],\n",
       "          [ 4,  5, 22],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4,  5, 22],\n",
       "          [22, -1, -1]],\n",
       " \n",
       "         [[ 4, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 14, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4,  7, -1],\n",
       "          [ 4, 14, -1]],\n",
       " \n",
       "         [[ 4, 22, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4,  7, -1],\n",
       "          [ 4,  7, -1],\n",
       "          [ 4, 22, -1]],\n",
       " \n",
       "         [[14, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [25, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [14, -1, -1]],\n",
       " \n",
       "         [[ 4, -1, -1],\n",
       "          [ 4, 14, 22],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 14, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [ 4, 14, -1]],\n",
       " \n",
       "         [[ 7, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [32, -1, -1]]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dataset.key_indices, raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gdal\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elimination_patch_list = []  \n",
    "for file_path in [cloud_shadow_file, snow_file]:\n",
    "    if not os.path.exists(file_path):\n",
    "        print('ERROR: file located at', file_path, 'does not exist')\n",
    "        exit()\n",
    "    with open(file_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            elimination_patch_list.append(row[0])\n",
    "#print('INFO:', len(elimination_patch_list), 'number of patches will be eliminated')\n",
    "elimination_patch_list = set(elimination_patch_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_patches = [patch for patch in patches if patch not in elimination_patch_list]\n",
    "len(filtered_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(data_dir, patches[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.randint(0, len(patches))\n",
    "all_bands = ['B01', 'B02', 'B03', 'B04', 'B05',\n",
    "              'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "rgb_bands = ['B04', 'B03', 'B02']\n",
    "\n",
    "band_stack = []\n",
    "for bands in rgb_bands:\n",
    "    band_path = os.path.join(data_dir, patches[idx], \"{}_{}.tif\".format(patches[idx], bands))\n",
    "    print(\"Loading\", band_path)\n",
    "    assert os.path.isfile(band_path)\n",
    "\n",
    "    band_ds = gdal.Open(band_path,  gdal.GA_ReadOnly)\n",
    "    raster_band = band_ds.GetRasterBand(1)\n",
    "    band_data = raster_band.ReadAsArray()\n",
    "    band_stack.append(band_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_OPTICAL_MAX_VALUE = 2000. # magic number from some google guys\n",
    "img = np.stack(band_stack) / _OPTICAL_MAX_VALUE # (C, W, H)\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(np.transpose(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, patches[idx], \"{}_labels_metadata.json\".format(patches[idx])), 'r') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "c.update(metadata['labels'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "label_counts = Counter()\n",
    "for patch in tqdm(patches):\n",
    "    with open(os.path.join(data_dir, patch, \"{}_labels_metadata.json\".format(patch)), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "        label_counts.update(metadata['labels'])\n",
    "print(label_counts)\n",
    "\n",
    "with open(\"label_counts_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_counts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(label_counts.keys(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
