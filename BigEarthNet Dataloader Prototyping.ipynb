{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"../SmallEarthNet/\"\n",
    "cloud_shadow_file = \"../patches_with_cloud_and_shadow.csv\"\n",
    "snow_file = \"../patches_with_seasonal_snow.csv\"\n",
    "patches = os.listdir(data_dir)\n",
    "patches.sort()\n",
    "print(len(patches))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import load_data_tf as load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data.BigEarthNetDataset(split_file=\"splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Mixed forest': 86198, 'Non-irrigated arable land': 85512, 'Coniferous forest': 81971, 'Broad-leaved forest': 72487, 'Transitional woodland/shrub': 69834, 'Land principally occupied by agriculture, with significant areas of natural vegetation': 69389, 'Complex cultivation patterns': 54184, 'Pastures': 44475, 'Sea and ocean': 37827, 'Discontinuous urban fabric': 31484, 'Water bodies': 29301, 'Agro-forestry areas': 8736, 'Continuous urban fabric': 7682, 'Peatbogs': 7072, 'Natural grassland': 6832, 'Industrial or commercial units': 4982, 'Water courses': 4185, 'Sclerophyllous vegetation': 4150, 'Vineyards': 3603, 'Permanently irrigated land': 3535, 'Olive groves': 3229, 'Inland marshes': 2495, 'Sport and leisure facilities': 2205, 'Annual crops associated with permanent crops': 2096, 'Mineral extraction sites': 1990, 'Bare rock': 1975, 'Fruit trees and berry plantations': 1921, 'Moors and heathland': 1754, 'Road and rail networks and associated land': 1460, 'Rice fields': 940, 'Green urban areas': 705, 'Sparsely vegetated areas': 623, 'Salt marshes': 504, 'Beaches, dunes, sands': 503, 'Construction sites': 500, 'Coastal lagoons': 399, 'Dump sites': 397, 'Airports': 359, 'Intertidal flats': 313, 'Estuaries': 301, 'Port areas': 185, 'Salines': 134, 'Burnt areas': 105})\n",
      "{0: 'Agro-forestry areas', 1: 'Airports', 2: 'Annual crops associated with permanent crops', 3: 'Bare rock', 4: 'Beaches, dunes, sands', 5: 'Broad-leaved forest', 6: 'Burnt areas', 7: 'Coastal lagoons', 8: 'Complex cultivation patterns', 9: 'Coniferous forest', 10: 'Construction sites', 11: 'Continuous urban fabric', 12: 'Discontinuous urban fabric', 13: 'Dump sites', 14: 'Estuaries', 15: 'Fruit trees and berry plantations', 16: 'Green urban areas', 17: 'Industrial or commercial units', 18: 'Inland marshes', 19: 'Intertidal flats', 20: 'Land principally occupied by agriculture, with significant areas of natural vegetation', 21: 'Mineral extraction sites', 22: 'Mixed forest', 23: 'Moors and heathland', 24: 'Natural grassland', 25: 'Non-irrigated arable land', 26: 'Olive groves', 27: 'Pastures', 28: 'Peatbogs', 29: 'Permanently irrigated land', 30: 'Port areas', 31: 'Rice fields', 32: 'Road and rail networks and associated land', 33: 'Salines', 34: 'Salt marshes', 35: 'Sclerophyllous vegetation', 36: 'Sea and ocean', 37: 'Sparsely vegetated areas', 38: 'Sport and leisure facilities', 39: 'Transitional woodland/shrub', 40: 'Vineyards', 41: 'Water bodies', 42: 'Water courses'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "738532"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(meta_dataset.counts)\n",
    "print(meta_dataset.dataset.idx_to_label)\n",
    "sum(meta_dataset.counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1859/9607 [00:00<00:00, 18581.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading train-val-test split cache from smallearthnet_splits.pkl\n",
      "File smallearthnet_splits.pkl not found. Creating new split instead with file name smallearthnet.pkl\n",
      "Building new train-val-test split and saving to smallearthnet.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9607/9607 [00:00<00:00, 17363.29it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_dataset = load_data.MetaBigEarthNetTaskDataset(data_dir=\"../SmallEarthNet\", split_save_path=\"smallearthnet.pkl\", split_file=\"smallearthnet_splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 19, 20, 22, 24, 25, 28, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42] 2965\n",
      "[[[0. 0. 1.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [1. 0. 1.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]]] [[[ 7. -1. -1.]\n",
      "  [ 4.  6. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]]\n",
      "\n",
      " [[ 4.  7. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [14. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]]\n",
      "\n",
      " [[ 7. -1. -1.]\n",
      "  [14. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 7. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [14. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4.  7. -1.]]\n",
      "\n",
      " [[ 4. -1. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 4.  5. -1.]\n",
      "  [ 4.  7. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 4. -1. -1.]\n",
      "  [ 7. -1. -1.]]]\n"
     ]
    }
   ],
   "source": [
    "X, y, yt = meta_dataset.sample_batch(batch_size=4, split='train')\n",
    "print(y, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 120, 120]), torch.Size([16, 43]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = DataLoader(data, batch_size=16)\n",
    "img_batch, label_batch = next(iter(train))\n",
    "img_batch.size(), label_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading train-val-test split cache from splits.pkl\n",
      "Reloading train-val-test split cache from splits.pkl\n",
      "Reloading train-val-test split cache from splits.pkl\n"
     ]
    }
   ],
   "source": [
    "train, _, _ = load_data.get_dataloaders(split_file=\"splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8, 3, 120, 120]), torch.Size([8, 8, 3]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch, label_batch, raw_labels = next(iter(train))\n",
    "img_batch.size(), label_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  19,\n",
       "  20,\n",
       "  22,\n",
       "  24,\n",
       "  25,\n",
       "  28,\n",
       "  30,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42],\n",
       " tensor([[[22, -1, -1],\n",
       "          [ 4, 17, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [17, 22, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, 22, -1]],\n",
       " \n",
       "         [[ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, 37, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [22, -1, -1],\n",
       "          [ 4, 22, -1]],\n",
       " \n",
       "         [[ 4,  5, 22],\n",
       "          [ 4,  5, -1],\n",
       "          [ 4,  5, 22],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4,  5, 22],\n",
       "          [22, -1, -1]],\n",
       " \n",
       "         [[ 4, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 14, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4,  7, -1],\n",
       "          [ 4, 14, -1]],\n",
       " \n",
       "         [[ 4, 22, -1],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4,  7, -1],\n",
       "          [ 4,  7, -1],\n",
       "          [ 4, 22, -1]],\n",
       " \n",
       "         [[14, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [25, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [14, -1, -1]],\n",
       " \n",
       "         [[ 4, -1, -1],\n",
       "          [ 4, 14, 22],\n",
       "          [ 4, 22, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [ 4, 14, -1],\n",
       "          [ 4, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [ 4, 14, -1]],\n",
       " \n",
       "         [[ 7, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [14, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [32, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [ 7, -1, -1],\n",
       "          [32, -1, -1]]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dataset.key_indices, raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gdal\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elimination_patch_list = []  \n",
    "for file_path in [cloud_shadow_file, snow_file]:\n",
    "    if not os.path.exists(file_path):\n",
    "        print('ERROR: file located at', file_path, 'does not exist')\n",
    "        exit()\n",
    "    with open(file_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            elimination_patch_list.append(row[0])\n",
    "#print('INFO:', len(elimination_patch_list), 'number of patches will be eliminated')\n",
    "elimination_patch_list = set(elimination_patch_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_patches = [patch for patch in patches if patch not in elimination_patch_list]\n",
    "len(filtered_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(data_dir, patches[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.randint(0, len(patches))\n",
    "all_bands = ['B01', 'B02', 'B03', 'B04', 'B05',\n",
    "              'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "rgb_bands = ['B04', 'B03', 'B02']\n",
    "\n",
    "band_stack = []\n",
    "for bands in rgb_bands:\n",
    "    band_path = os.path.join(data_dir, patches[idx], \"{}_{}.tif\".format(patches[idx], bands))\n",
    "    print(\"Loading\", band_path)\n",
    "    assert os.path.isfile(band_path)\n",
    "\n",
    "    band_ds = gdal.Open(band_path,  gdal.GA_ReadOnly)\n",
    "    raster_band = band_ds.GetRasterBand(1)\n",
    "    band_data = raster_band.ReadAsArray()\n",
    "    band_stack.append(band_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_OPTICAL_MAX_VALUE = 2000. # magic number from some google guys\n",
    "img = np.stack(band_stack) / _OPTICAL_MAX_VALUE # (C, W, H)\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(np.transpose(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, patches[idx], \"{}_labels_metadata.json\".format(patches[idx])), 'r') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "c.update(metadata['labels'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "label_counts = Counter()\n",
    "for patch in tqdm(patches):\n",
    "    with open(os.path.join(data_dir, patch, \"{}_labels_metadata.json\".format(patch)), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "        label_counts.update(metadata['labels'])\n",
    "print(label_counts)\n",
    "\n",
    "with open(\"label_counts_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_counts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(label_counts.keys(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
